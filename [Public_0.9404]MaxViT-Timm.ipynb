{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install albumentations timm pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import torch\n",
    "import cv2\n",
    "import timm\n",
    "import random\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm.auto import notebook_tqdm\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, mode='train', transform=None):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "\n",
    "        if mode == 'train':\n",
    "            self.label_encoder = LabelEncoder()\n",
    "            self.df['label'] = self.label_encoder.fit_transform(self.df['label'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir, self.df.iloc[idx, 0])\n",
    "        image = Image.open(img_name)\n",
    "        image = np.array(image)\n",
    "        \n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "            \n",
    "        label = self.df.iloc[idx, 2] if self.mode == 'train' else -1\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(224, 224, interpolation=cv2.INTER_CUBIC),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.Normalize(mean=mean, std=std),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(224, 224, interpolation=cv2.INTER_CUBIC),\n",
    "    A.Normalize(mean=mean, std=std),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(csv_file=\"./data/train.csv\", root_dir=\"./data/\", mode='train', transform=train_transform)\n",
    "test_dataset = CustomDataset(csv_file=\"./data/test.csv\", root_dir=\"./data/\", mode='test', transform=test_transform)\n",
    "\n",
    "total_train_samples = len(train_dataset)\n",
    "val_size = int(0.1 * total_train_samples)\n",
    "train_size = total_train_samples - val_size\n",
    "\n",
    "train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True, num_workers=12)\n",
    "val_loader = DataLoader(val_subset, batch_size=32, shuffle=False, num_workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model(\"timm/maxvit_large_tf_224.in1k\", pretrained=True, num_classes=25)\n",
    "model.to(device)\n",
    "model = torch.nn.DataParallel(model)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True, min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, true):\n",
    "  _, preds = torch.max(pred, dim=1)\n",
    "  return torch.tensor(torch.sum(preds == true).item() / len(preds))\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "\n",
    "    tqdm_bar = notebook_tqdm(dataloader, desc='Training')\n",
    "    for batch_idx, (images, labels) in enumerate(tqdm_bar):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc += accuracy(outputs, labels).item()\n",
    "\n",
    "        avg_loss = train_loss / (batch_idx + 1)\n",
    "        avg_acc = train_acc / (batch_idx + 1)\n",
    "        \n",
    "        tqdm_bar.set_postfix(\n",
    "            {'loss': f'{avg_loss:.5f}',\n",
    "            'accuracy': f'{avg_acc:.5f}'}\n",
    "        )\n",
    "    return train_loss / len(dataloader), train_acc / len(dataloader)\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "\n",
    "    tqdm_bar = notebook_tqdm(dataloader, desc='Training')\n",
    "    for batch_idx, (images, labels) in enumerate(tqdm_bar):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        val_loss += loss.item()\n",
    "        val_acc += accuracy(outputs, labels).item()\n",
    "        \n",
    "        avg_loss = val_loss / (batch_idx + 1)\n",
    "        avg_acc = val_acc / (batch_idx + 1)\n",
    "        \n",
    "        tqdm_bar.set_postfix(\n",
    "            {'loss': f'{avg_loss:.5f}',\n",
    "            'accuracy': f'{avg_acc:.5f}'}\n",
    "        )\n",
    "    return val_loss / len(dataloader), val_acc / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_weight_path = \"./weights/maxvit-v1-best.pth\"\n",
    "current_weight_path = \"./weights/maxvit-v1-current.pth\"\n",
    "\n",
    "patience = 10\n",
    "num_epochs = 100\n",
    "\n",
    "if os.path.exists(current_weight_path):\n",
    "    checkpoint = torch.load(current_weight_path)\n",
    "    best_checkpoint = torch.load(best_weight_path)\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    best_val_loss = best_checkpoint['best_val_loss']\n",
    "    early_stopping_counter = checkpoint['early_stopping_counter']\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print('Loaded model from last checkpoint')\n",
    "    print(\"Last best validation loss: \", best_val_loss)\n",
    "    print(\"Continuing from epoch: \", start_epoch)\n",
    "    print(\"Early stopping counter: \", early_stopping_counter)\n",
    "else:\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, num_epochs):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, LR: {current_lr}\")\n",
    "    print(f\"Train Loss: {train_loss:.5f}, Train Accuracy: {train_acc:.5f}\")\n",
    "    print(f\"Val Loss: {val_loss:.5f}, Val Accuracy: {val_acc:.5f}\")\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    current_checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_loss': val_loss,\n",
    "        'early_stopping_counter': early_stopping_counter\n",
    "    }\n",
    "\n",
    "    torch.save(current_checkpoint, current_weight_path)\n",
    "    print('Model saved')\n",
    "\n",
    "    ''' Save the weights with the best validation loss '''\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'early_stopping_counter': early_stopping_counter\n",
    "        }\n",
    "\n",
    "        torch.save(best_checkpoint, best_weight_path)\n",
    "        early_stopping_counter = 0\n",
    "        print('Best model saved')\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(f'Early stopping at epoch: {epoch+1}')\n",
    "            break\n",
    "print('Training finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import timm\n",
    "import os\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "from tqdm.auto import notebook_tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./weights/maxvit-v1-best.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, mode='train', transform=None):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "\n",
    "        if mode == 'train':\n",
    "            self.label_encoder = LabelEncoder()\n",
    "            self.df['label'] = self.label_encoder.fit_transform(self.df['label'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir, self.df.iloc[idx, 0])\n",
    "        image = Image.open(img_name)\n",
    "        image = np.array(image)\n",
    "        \n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "            \n",
    "        label = self.df.iloc[idx, 2] if self.mode == 'train' else -1\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(224, 224, interpolation=cv2.INTER_CUBIC),\n",
    "    A.Normalize(mean=mean, std=std),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(csv_file=\"./data/test.csv\", root_dir=\"./data/test/\", mode='test', transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "model = timm.create_model(\"timm/maxvit_large_tf_224.in1k\", pretrained=False, num_classes=25)\n",
    "model.to(device)\n",
    "model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "def test(model, dataloder):\n",
    "    predictions = []\n",
    "    model.eval()\n",
    "\n",
    "    tqdm_bar = notebook_tqdm(dataloder, desc=\"Predicting\")\n",
    "    for images, _ in tqdm_bar:\n",
    "        images = images.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = test(model, test_loader)\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "train_df = pd.read_csv(\"./data/train_df.csv\")\n",
    "le.fit(train_df[\"label\"])\n",
    "\n",
    "final_prediction = le.inverse_transform(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.read_csv(\"./data/sample_submission.csv\")\n",
    "submission_df['label'] = final_prediction\n",
    "submission_df.to_csv(\"./answer.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
